{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5500237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add49474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa5a3239",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOUND_SAMPLE_LENGTH = 3000000\n",
    "\n",
    "HAMMING_SIZE = 100\n",
    "HAMMING_STRIDE = 40\n",
    "def preprocessingAudio(audioPath):\n",
    "    print ('Prepossessing ' + audioPath)\n",
    "\n",
    "   \n",
    "    y, sr = librosa.load(audioPath)\n",
    "\n",
    "    # Let's make and display a mel-scaled power (energy-squared) spectrogram\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
    "\n",
    "    # Convert to log scale (dB). We'll use the peak power as reference.\n",
    "    log_S = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "    mfcc = librosa.feature.mfcc(S=log_S, sr=sr, n_mfcc=13)\n",
    "    # featuresArray.append(mfcc)\n",
    "\n",
    "    \n",
    "    spectrogram_db = librosa.power_to_db(S, ref=np.max)\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     librosa.display.specshow(spectrogram_db, y_axis='mel', x_axis='time', sr=sr, hop_length=512)\n",
    "#     plt.colorbar(format='%+2.0f dB')\n",
    "#     plt.title('Mel Spectrogram')\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    return spectrogram_db[:, :2500]\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13d6d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import ImageFolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97de4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectrogramRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SpectrogramRegressor, self).__init__()\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(32 * (input_height // 4) * (input_width // 4), 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 6)  # Regression output is a single value\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c15f68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSpectrogramDataset(Dataset):\n",
    "    def __init__(self, images, values, transform=None):\n",
    "        self.images = images\n",
    "        self.values = values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        value = self.values[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b27cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 1\n",
    "learning_rate = .001\n",
    "input_height, input_width = (128,2500)  # Dimensions of your spectrogram images\n",
    "\n",
    "# Create the model instance\n",
    "model = SpectrogramRegressor()\n",
    "\n",
    "# Define loss function and optimizer for regression\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Load and preprocess dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((input_height, input_width)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f001c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Name  Intonation  Dynamics  Note_Accuracy  Expressiveness  \\\n",
      "0    Daphnis1           8         7              4               7   \n",
      "1    Daphnis2           9         8              6              10   \n",
      "2    Daphnis3           5         3              5               2   \n",
      "3    Daphnis4           6         3              5               8   \n",
      "4    Daphnis5           7         6             10               7   \n",
      "5    Daphnis6           9        10             10              10   \n",
      "6    Daphnis7           8         7              6               8   \n",
      "7    Daphnis8          10        10             10              10   \n",
      "8    Daphnis9           9        10             10               8   \n",
      "9   Daphnis10           8        10             10               7   \n",
      "10  Daphnis11           4         3              5               2   \n",
      "11  Daphnis12          10         9             10              10   \n",
      "12  Daphnis13           7         5              8               4   \n",
      "13  Daphnis14           6         4             10               6   \n",
      "14  Daphnis15           2         2              3               3   \n",
      "15  Daphnis16           6         4             10               8   \n",
      "16  Daphnis17          10        10              9              10   \n",
      "17  Daphnis18          10        10             10               9   \n",
      "18  Daphnis19           7         6             10              10   \n",
      "19  Daphnis20          10        10              9               7   \n",
      "20  Daphnis21           8         9             10              10   \n",
      "21  Daphnis22           9        10             10              10   \n",
      "22  Daphnis23           7         5              8               7   \n",
      "23  Daphnis24          10        10             10               9   \n",
      "24  Daphnis25           7         7             10               5   \n",
      "25  Daphnis26           7         6             10               5   \n",
      "26  Daphnis27           7         9             10              10   \n",
      "27  Daphnis28           8        10             10              10   \n",
      "28  Daphnis29           7         7              9               8   \n",
      "29  Daphnis30          10         9              9               9   \n",
      "30  Daphnis31           9         8             10              10   \n",
      "31  Daphnis32           8         9             10               7   \n",
      "32  Daphnis33           4         5              6               3   \n",
      "33  Daphnis34           5         4              4               3   \n",
      "34  Daphnis35           7         7              8               7   \n",
      "35  Daphnis36           8         8             10               7   \n",
      "36  Daphnis37           7         9              9               4   \n",
      "37  Daphnis38           6         5              5               3   \n",
      "38  Daphnis39           8         8              9               9   \n",
      "39  Daphnis40           8         9             10               8   \n",
      "\n",
      "    Articulation  Tone_Quality  \n",
      "0             10             5  \n",
      "1             10             7  \n",
      "2              7             2  \n",
      "3              9             5  \n",
      "4              8             7  \n",
      "5             10            10  \n",
      "6              5             6  \n",
      "7             10             9  \n",
      "8             10             6  \n",
      "9             10             9  \n",
      "10             6             2  \n",
      "11            10            10  \n",
      "12            10             3  \n",
      "13             8             5  \n",
      "14             5             2  \n",
      "15            10             8  \n",
      "16            10             7  \n",
      "17            10            10  \n",
      "18            10             7  \n",
      "19            10             6  \n",
      "20            10             8  \n",
      "21            10            10  \n",
      "22             8             6  \n",
      "23            10             9  \n",
      "24             8             5  \n",
      "25             9             7  \n",
      "26            10            10  \n",
      "27             9             8  \n",
      "28             8             8  \n",
      "29            10             7  \n",
      "30            10             9  \n",
      "31            10             9  \n",
      "32             4             2  \n",
      "33             3             3  \n",
      "34             7             6  \n",
      "35             7             6  \n",
      "36             8             7  \n",
      "37             4             3  \n",
      "38            10             9  \n",
      "39            10             8  \n",
      "Daphnis12.mp3\n",
      "Daphnis12\n",
      "10\n",
      "Prepossessing Daphnis12.mp3\n",
      "Daphnis11.mp3\n",
      "Daphnis11\n",
      "4\n",
      "Prepossessing Daphnis11.mp3\n",
      "Daphnis39.mp3\n",
      "Daphnis39\n",
      "8\n",
      "Prepossessing Daphnis39.mp3\n",
      "Daphnis38.mp3\n",
      "Daphnis38\n",
      "6\n",
      "Prepossessing Daphnis38.mp3\n",
      "Daphnis10.mp3\n",
      "Daphnis10\n",
      "8\n",
      "Prepossessing Daphnis10.mp3\n",
      "Daphnis28.mp3\n",
      "Daphnis28\n",
      "8\n",
      "Prepossessing Daphnis28.mp3\n",
      "Daphnis29.mp3\n",
      "Daphnis29\n",
      "7\n",
      "Prepossessing Daphnis29.mp3\n",
      "Daphnis17.mp3\n",
      "Daphnis17\n",
      "10\n",
      "Prepossessing Daphnis17.mp3\n",
      "Daphnis8.mp3\n",
      "Daphnis8\n",
      "10\n",
      "Prepossessing Daphnis8.mp3\n",
      "Daphnis9.mp3\n",
      "Daphnis9\n",
      "9\n",
      "Prepossessing Daphnis9.mp3\n",
      "Daphnis1.mp3\n",
      "Daphnis1\n",
      "8\n",
      "Prepossessing Daphnis1.mp3\n",
      "Daphnis2.mp3\n",
      "Daphnis2\n",
      "9\n",
      "Prepossessing Daphnis2.mp3\n",
      "Daphnis6.mp3\n",
      "Daphnis6\n",
      "9\n",
      "Prepossessing Daphnis6.mp3\n",
      "Daphnis40.mp3\n",
      "Daphnis40\n",
      "8\n",
      "Prepossessing Daphnis40.mp3\n",
      "Daphnis33.mp3\n",
      "Daphnis33\n",
      "4\n",
      "Prepossessing Daphnis33.mp3\n",
      "Daphnis27.mp3\n",
      "Daphnis27\n",
      "7\n",
      "Prepossessing Daphnis27.mp3\n",
      "Daphnis26.mp3\n",
      "Daphnis26\n",
      "7\n",
      "Prepossessing Daphnis26.mp3\n",
      "Daphnis32.mp3\n",
      "Daphnis32\n",
      "8\n",
      "Prepossessing Daphnis32.mp3\n",
      "Daphnis24.mp3\n",
      "Daphnis24\n",
      "10\n",
      "Prepossessing Daphnis24.mp3\n",
      "Daphnis30.mp3\n",
      "Daphnis30\n",
      "10\n",
      "Prepossessing Daphnis30.mp3\n",
      "Daphnis18.mp3\n",
      "Daphnis18\n",
      "10\n",
      "Prepossessing Daphnis18.mp3\n",
      "Daphnis31.mp3\n",
      "Daphnis31\n",
      "9\n",
      "Prepossessing Daphnis31.mp3\n",
      "Daphnis25.mp3\n",
      "Daphnis25\n",
      "7\n",
      "Prepossessing Daphnis25.mp3\n",
      "Daphnis21.mp3\n",
      "Daphnis21\n",
      "8\n",
      "Prepossessing Daphnis21.mp3\n",
      "Daphnis35.mp3\n",
      "Daphnis35\n",
      "7\n",
      "Prepossessing Daphnis35.mp3\n",
      "Daphnis34.mp3\n",
      "Daphnis34\n",
      "5\n",
      "Prepossessing Daphnis34.mp3\n",
      "Daphnis20.mp3\n",
      "Daphnis20\n",
      "10\n",
      "Prepossessing Daphnis20.mp3\n",
      "Daphnis36.mp3\n",
      "Daphnis36\n",
      "8\n",
      "Prepossessing Daphnis36.mp3\n",
      "Daphnis22.mp3\n",
      "Daphnis22\n",
      "9\n",
      "Prepossessing Daphnis22.mp3\n",
      "Daphnis23.mp3\n",
      "Daphnis23\n",
      "7\n",
      "Prepossessing Daphnis23.mp3\n",
      "(30, 128, 2500)\n",
      "(30, 6)\n"
     ]
    }
   ],
   "source": [
    "filepath = \"Excerpts7.csv\"\n",
    "df = pd.read_csv(filepath,header = 0)\n",
    "print(df)\n",
    "list_of_intonations = []\n",
    "list_of_spectrograms = []\n",
    "for soundfile in os.listdir(os.getcwd()):\n",
    "    if soundfile[-3:] == \"mp3\":\n",
    "        print(soundfile)\n",
    "        file_prefix = soundfile[:soundfile.index(\".\")]\n",
    "        print(file_prefix)\n",
    "        index = df.index[df['Name'] == file_prefix].tolist()[0]\n",
    "        intonation=df.iloc[index]['Intonation']\n",
    "        dynamics=df.iloc[index]['Dynamics']\n",
    "        note_accuracy=df.iloc[index]['Note_Accuracy']\n",
    "        expressiveness=df.iloc[index]['Expressiveness']\n",
    "        articulation=df.iloc[index]['Articulation']\n",
    "        tone_quality=df.iloc[index]['Tone_Quality']\n",
    "            \n",
    "        print(intonation)\n",
    "        list_of_intonations += [[intonation,dynamics,note_accuracy,expressiveness,articulation,tone_quality]]\n",
    "        spectrogram = preprocessingAudio(soundfile)\n",
    "        #print(spectrogram)\n",
    "        #print(spectrogram.shape)\n",
    "        list_of_spectrograms += [spectrogram]\n",
    "stacked_arrays = np.stack(list_of_spectrograms)\n",
    "print(stacked_arrays.shape)\n",
    "list_of_intonations = np.array(list_of_intonations)\n",
    "print(list_of_intonations.shape)\n",
    "    #prepossessingAudio(\"Daphnis1.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c7b5e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform numpy arrays to tensors\n",
    "images = torch.tensor(stacked_arrays, dtype=torch.float32)\n",
    "values = torch.tensor(list_of_intonations, dtype=torch.float32)\n",
    "\n",
    "# Create a custom dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),  # Convert tensor to PIL image\n",
    "    transforms.Resize((input_height, input_width)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = CustomSpectrogramDataset(images, values, transform=transform)\n",
    "from torch.utils.data import random_split\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.7 * dataset_size)  # 70% for training\n",
    "val_size = int(0.15 * dataset_size)   # 15% for validation\n",
    "test_size = dataset_size - train_size - val_size  # Remaining for testing\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2ed3994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 38.5839\n",
      "Epoch [1/20], Validated loss: 7.0685\n",
      "Epoch [2/20], Loss: 6.2680\n",
      "Epoch [2/20], Validated loss: 6.9386\n",
      "Epoch [3/20], Loss: 4.8675\n",
      "Epoch [3/20], Validated loss: 6.0951\n",
      "Epoch [4/20], Loss: 5.6073\n",
      "Epoch [4/20], Validated loss: 6.5643\n",
      "Epoch [5/20], Loss: 4.5337\n",
      "Epoch [5/20], Validated loss: 7.1646\n",
      "Epoch [6/20], Loss: 4.8210\n",
      "Epoch [6/20], Validated loss: 10.4092\n",
      "Epoch [7/20], Loss: 3.3327\n",
      "Epoch [7/20], Validated loss: 7.5238\n",
      "Epoch [8/20], Loss: 1.6142\n",
      "Epoch [8/20], Validated loss: 9.7218\n",
      "Epoch [9/20], Loss: 1.3046\n",
      "Epoch [9/20], Validated loss: 7.0423\n",
      "Epoch [10/20], Loss: 1.1593\n",
      "Epoch [10/20], Validated loss: 8.1491\n",
      "Epoch [11/20], Loss: 0.8485\n",
      "Epoch [11/20], Validated loss: 8.6935\n",
      "Epoch [12/20], Loss: 0.4695\n",
      "Epoch [12/20], Validated loss: 9.1516\n",
      "Epoch [13/20], Loss: 0.5355\n",
      "Epoch [13/20], Validated loss: 9.6553\n",
      "Epoch [14/20], Loss: 0.2532\n",
      "Epoch [14/20], Validated loss: 10.2735\n",
      "Epoch [15/20], Loss: 0.3464\n",
      "Epoch [15/20], Validated loss: 7.9627\n",
      "Epoch [16/20], Loss: 0.4031\n",
      "Epoch [16/20], Validated loss: 8.6983\n",
      "Epoch [17/20], Loss: 0.3038\n",
      "Epoch [17/20], Validated loss: 9.3172\n",
      "Epoch [18/20], Loss: 0.4046\n",
      "Epoch [18/20], Validated loss: 9.7601\n",
      "Epoch [19/20], Loss: 0.3620\n",
      "Epoch [19/20], Validated loss: 12.3225\n",
      "Epoch [20/20], Loss: 1.3598\n",
      "Epoch [20/20], Validated loss: 6.5870\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "min_val_loss = 1000000\n",
    "for epoch in range(num_epochs):\n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    for images, labels in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.float())  # Convert labels to float for regression\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_losses += [loss.item()]\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {sum(training_losses)/len(training_losses):.4f}')\n",
    "    for images,labels in val_dataloader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels.float())  # Convert labels to float for regression\n",
    "        validation_losses += [loss.item()]\n",
    "    avg_val_loss = sum(validation_losses)/len(validation_losses)\n",
    "    if avg_val_loss < min_val_loss:\n",
    "        min_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), 'spectrogram_regressor.pth')\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Validated loss: {sum(validation_losses)/len(validation_losses):.4f}') \n",
    "# Save the trained model\n",
    "#torch.save(model.state_dict(), 'spectrogram_regressor.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f5a44b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.5589, 7.7046, 7.8953, 6.9671, 7.8390, 6.6785]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[10., 10., 10.,  9., 10., 10.]])\n",
      "tensor([[7.3837, 7.5762, 7.9376, 6.6399, 7.7180, 6.5907]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[4., 3., 5., 2., 6., 2.]])\n",
      "tensor([[7.1121, 7.4152, 7.9709, 6.8275, 7.8637, 6.4789]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[7., 7., 9., 8., 8., 8.]])\n",
      "tensor([[7.3874, 7.5207, 8.0014, 6.6458, 7.7444, 6.5985]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 9., 10., 10., 10., 10., 10.]])\n",
      "tensor([[7.4038, 7.6762, 8.0885, 6.8938, 8.0887, 6.8910]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor([[ 9., 10., 10.,  8., 10.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "intonation_error = []\n",
    "dynamics_error = []\n",
    "note_accuracy_error = []\n",
    "express_error = []\n",
    "articulation_error = []\n",
    "tone_quality_error = []\n",
    "for images, labels in test_dataloader:\n",
    "        outputs = model(images)\n",
    "        outputs_lists = outputs.tolist()\n",
    "        labels_list = labels.tolist()\n",
    "        intonation_error += [abs(outputs_lists[0][0]-labels_list[0][0])]\n",
    "        dynamics_error = [abs(outputs_lists[0][1]-labels_list[0][1])]\n",
    "        note_accuracy_error = [abs(outputs_lists[0][2]-labels_list[0][2])]\n",
    "        express_error = [abs(outputs_lists[0][3]-labels_list[0][3])]\n",
    "        articulation_error = [abs(outputs_lists[0][4]-labels_list[0][4])]\n",
    "        tone_quality_error = [abs(outputs_lists[0][5]-labels_list[0][5])]\n",
    "        print(outputs)\n",
    "        print(labels)\n",
    "avg_intonation_error = np.mean(intonation_error)\n",
    "avg_dynamics_error = np.mean(dynamics_error)\n",
    "avg_note_accuracy_error = np.mean(note_accuracy_error)\n",
    "avg_express_error=np.mean(express_error)\n",
    "avg_articulation_error=np.mean(articulation_error)\n",
    "avg_tone_quality_error =np.mean(tone_quality_error)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb8081e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/manu/anaconda3/lib/python3.11/site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in /Users/manu/anaconda3/lib/python3.11/site-packages (from openai) (2.29.0)\n",
      "Requirement already satisfied: tqdm in /Users/manu/anaconda3/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /Users/manu/anaconda3/lib/python3.11/site-packages (from openai) (3.8.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/manu/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/manu/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/manu/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/manu/anaconda3/lib/python3.11/site-packages (from requests>=2.20->openai) (2023.7.22)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/manu/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/manu/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/manu/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/manu/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/manu/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/manu/anaconda3/lib/python3.11/site-packages (from aiohttp->openai) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b116938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "messages = [\n",
    " \n",
    "    {\"role\": \"system\", \"content\": \"\"\"You are a helpful and kind AI Assistant. I will be giving you 6 values. the first is intonation, the second is dynamics, the third is note accuracy, the fourth is expressiveness,\n",
    "     the fifth is articulation, the sixth is tone quality. I am a flute performer playing the flute solo from rehersal number 176-179 in Daphnis et Chloe by Ravel. This numbers are evaluations of my playing out of 10. Give me feeback on how I can improve my performance of this piece in each category relative to the score I recieved. Please provide references to this composition in your feedback. Be very specific to the excerpt such as maintaining good pitch in the high register or being able to remain quiet even in the high register. If the intonation is low mention to watch the pitch on the high G# because many players tend to play it sharp. If the tone quality is below a 7 mention to keep a round ambechure and open throat, and mention to have a more spinning vibrato. \"\"\"},\n",
    "]\n",
    "def chatbot(input):\n",
    "    if input:\n",
    "        messages.append({\"role\": \"user\", \"content\": input})\n",
    "        chat = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages\n",
    "        )\n",
    "        reply = chat.choices[0].message.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4f98533",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = chatbot(str(outputs_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4780a3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intonation (7.40/10):\n",
      "Your intonation is generally good, but there are a few areas where you can improve. In measures 177-178, there are several high G#s that need to be played with caution. Make sure to watch the pitch on these notes, as many players tend to play them sharp. Use your embouchure and air support to bring these notes in tune with the rest of the ensemble. Pay attention to the overall pitch tendencies in this section and make necessary adjustments to maintain good intonation.\n",
      "\n",
      "Dynamics (7.68/10):\n",
      "Your dynamics are well-controlled for the most part, but there are opportunities to bring out more contrast in this section. In measures 176-177, there is a piano marking, indicating a soft dynamic. Ensure that your sound remains controlled and delicate throughout these measures while still maintaining a clear tone. In measure 178, where the music begins to crescendo, make sure to gradually increase the volume while keeping the tone quality consistent. Pay attention to the marked dynamic changes and fully explore the dynamic range of this piece to create a more expressive performance.\n",
      "\n",
      "Note Accuracy (8.09/10):\n",
      "Your note accuracy is quite good, but there are a few areas where you can focus on enhancing your precision. In measure 176, make sure to play the high Bb accurately and with proper fingerings. Additionally, watch out for any rapid passages or challenging runs in this excerpt to ensure that each note is played with accuracy and clarity. Refer to the score and practice slowly to improve your note accuracy in these specific areas.\n",
      "\n",
      "Expressiveness (6.89/10):\n",
      "To increase your expressiveness in this section, focus on adding more interpretation and emotion to your performance. In measures 177-179, there are several opportunities to shape the melodic lines and bring out the phrasing and musicality. Pay attention to the dynamic markings, articulations, and expressive indications in the score. Experiment with varying your vibrato, dynamics, and articulation to create a more engaging and expressive interpretation of this solo.\n",
      "\n",
      "Articulation (8.09/10):\n",
      "Your articulation is generally clear and precise, but there are a few areas where you can improve. In measures 176-179, there are several staccato markings. Make sure to articulate each note cleanly and precisely, while still maintaining the overall flow of the music. Focus on using the appropriate tongue placement and air support to achieve a crisp and articulate sound. Practice these passages slowly and gradually increase the tempo to master the articulation and ensure its clarity throughout the excerpt.\n",
      "\n",
      "Tone Quality (6.89/10):\n",
      "While your tone quality is decent, there are ways to enhance it further. In this particular excerpt, strive for a round and resonant tone. Keep your embouchure firm yet flexible, maintaining an open throat and a consistent air stream. Pay close attention to your vibrato, ensuring it is spinning and controlled, adding depth and warmth to your tone. Experiment with different colors and timbres to express the character of this solo while maintaining a beautiful and consistent tone quality.\n",
      "\n",
      "Overall, your performance is commendable, but there are specific areas where you can focus on improvement. Practice with the score, paying attention to the specific markings and indications, and identify the technical and musical challenges of this excerpt. Take your time to address each aspect individually, and gradually integrate them into your overall performance.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e05ecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flute",
   "language": "python",
   "name": "flute"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
